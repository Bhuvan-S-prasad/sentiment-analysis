{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Sample:\n",
      "     id         game sentiment  \\\n",
      "0  2401  Borderlands  Positive   \n",
      "1  2401  Borderlands  Positive   \n",
      "2  2401  Borderlands  Positive   \n",
      "3  2401  Borderlands  Positive   \n",
      "4  2401  Borderlands  Positive   \n",
      "\n",
      "                                                text  \n",
      "0  im getting on borderlands and i will murder yo...  \n",
      "1  I am coming to the borders and I will kill you...  \n",
      "2  im getting on borderlands and i will kill you ...  \n",
      "3  im coming on borderlands and i will murder you...  \n",
      "4  im getting on borderlands 2 and i will murder ...  \n",
      "\n",
      "Validation Data Sample:\n",
      "     id       game   sentiment  \\\n",
      "0  3364   Facebook  Irrelevant   \n",
      "1   352     Amazon     Neutral   \n",
      "2  8312  Microsoft    Negative   \n",
      "3  4371      CS-GO    Negative   \n",
      "4  4433     Google     Neutral   \n",
      "\n",
      "                                                text  \n",
      "0  I mentioned on Facebook that I was struggling ...  \n",
      "1  BBC News - Amazon boss Jeff Bezos rejects clai...  \n",
      "2  @Microsoft Why do I pay for WORD when it funct...  \n",
      "3  CSGO matchmaking is so full of closet hacking,...  \n",
      "4  Now the President is slapping Americans in the...  \n",
      "\n",
      "Label distribution in training: sentiment\n",
      "Negative      22542\n",
      "Positive      20832\n",
      "Neutral       18318\n",
      "Irrelevant    12990\n",
      "Name: count, dtype: int64\n",
      "Label distribution in validation: sentiment\n",
      "Neutral       285\n",
      "Positive      277\n",
      "Negative      266\n",
      "Irrelevant    172\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values in training: id             0\n",
      "game           0\n",
      "sentiment      0\n",
      "text         686\n",
      "dtype: int64\n",
      "Missing values in validation: id           0\n",
      "game         0\n",
      "sentiment    0\n",
      "text         0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after handling:\n",
      "Training: id           0\n",
      "game         0\n",
      "sentiment    0\n",
      "text         0\n",
      "label        0\n",
      "dtype: int64\n",
      "Validation: id           0\n",
      "game         0\n",
      "sentiment    0\n",
      "text         0\n",
      "label        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\n",
    "    r'C:\\Users\\bhuva\\Desktop\\projects_2025\\LLM\\dataset\\twitter_training.csv', sep=',',\n",
    "    header=None,\n",
    "    names=['id', 'game', 'sentiment', 'text']\n",
    ")\n",
    "valid_df = pd.read_csv(\n",
    "    r'C:\\Users\\bhuva\\Desktop\\projects_2025\\LLM\\dataset\\twitter_validation.csv',\n",
    "    sep=',',  \n",
    "    header=None,\n",
    "    names=['id', 'game', 'sentiment', 'text']\n",
    ")\n",
    "\n",
    "# Display data samples\n",
    "print(\"Train Data Sample:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nValidation Data Sample:\")\n",
    "print(valid_df.head())\n",
    "\n",
    "# Check data distribution and quality\n",
    "print(\"\\nLabel distribution in training:\", train_df['sentiment'].value_counts())\n",
    "print(\"Label distribution in validation:\", valid_df['sentiment'].value_counts())\n",
    "\n",
    "#missing values\n",
    "print(\"\\nMissing values in training:\", train_df.isnull().sum())\n",
    "print(\"Missing values in validation:\", valid_df.isnull().sum())\n",
    "\n",
    "# Mapping sentiment labels to integers\n",
    "label2id = {'Positive': 1, 'Negative': 0, 'Neutral': 2, 'Irrelevant': 3}\n",
    "train_df['label'] = train_df['sentiment'].map(label2id)\n",
    "valid_df['label'] = valid_df['sentiment'].map(label2id)\n",
    "\n",
    "\n",
    "# Handle missing values\n",
    "train_df['text'] = train_df['text'].fillna(\"no text provided\")\n",
    "valid_df['text'] = valid_df['text'].fillna(\"no text provided\")\n",
    "\n",
    "\n",
    "print(\"\\nMissing values after handling:\")\n",
    "print(\"Training:\", train_df.isnull().sum())\n",
    "print(\"Validation:\", valid_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example from train_dataset:\n",
      "{'id': 2401, 'game': 'Borderlands', 'sentiment': 'Positive', 'text': 'im getting on borderlands and i will murder you all ,', 'label': 1}\n",
      "\n",
      "Dataset sizes:\n",
      "Training samples: 74682\n",
      "Validation samples: 1000\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Converts the pandas DataFrames into Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "\n",
    "\n",
    "print(\"\\nExample from train_dataset:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "print(\"\\nDataset sizes:\")\n",
    "print(\"Training samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9bd90f385c4894abef602a5fcef136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/74682 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6491e4d740d34344b76932f77912b21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Value\n",
    "\n",
    "#labels to int64\n",
    "train_dataset = train_dataset.cast_column(\"label\", Value(\"int64\"))\n",
    "valid_dataset = valid_dataset.cast_column(\"label\", Value(\"int64\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_tweet(text):\n",
    "    # Removes URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    # Removes Twitter handles\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Removes hash '#' symbol from hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "    # Removes numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Removes HTML entities\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    # Converts text to lowercase\n",
    "    text = text.lower()\n",
    "    # Removes punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Normalizes whitespace\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ac336e08644b148ea2391b57a45105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74682 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff7ca7e20c84b0b817270087f95174f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of processed training data:\n",
      "\n",
      "Example 1:\n",
      "Text: [CLS] im getting on borderlands and i will murder you all [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Label: 1\n",
      "\n",
      "Example 2:\n",
      "Text: [CLS] i am coming to the borders and i will kill you all [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Label: 1\n",
      "\n",
      "Example 3:\n",
      "Text: [CLS] im getting on borderlands and i will kill you all [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    texts = examples.get(\"text\", [])\n",
    "    cleaned_text = [preprocess_tweet(str(text)) for text in texts]\n",
    "    if \"label\" in examples:\n",
    "        examples[\"label\"] = [int(label) if not pd.isna(label) else -100 for label in examples[\"label\"]]\n",
    "    return tokenizer(cleaned_text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "#preprocessing the datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "print(\"\\nExample of processed training data:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(\"Text:\", tokenizer.decode(train_dataset[i]['input_ids']))\n",
    "    print(\"Label:\", train_dataset[i]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized and formatted training sample:\n",
      "{'label': tensor(1), 'input_ids': tensor([  101, 10047,  2893,  2006,  3675,  8653,  1998,  1045,  2097,  4028,\n",
      "         2017,  2035,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "columns_to_remove = [\"id\", \"game\", \"text\", \"sentiment\"]\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "valid_dataset = valid_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "# format for PyTorch\n",
    "train_dataset.set_format(\"torch\")\n",
    "valid_dataset.set_format(\"torch\")\n",
    "\n",
    "\n",
    "print(\"\\nTokenized and formatted training sample:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# class weights\n",
    "labels = train_df['label'].values\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.FloatTensor(class_weights)\n",
    "\n",
    "# Initializeing the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", \n",
    "    num_labels=len(label2id)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=r'C:\\Users\\bhuva\\Desktop\\projects_2025\\LLM\\output',\n",
    "    eval_strategy=\"epoch\",           \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=r'C:\\Users\\bhuva\\Desktop\\projects_2025\\LLM\\logging',\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",      \n",
    "    save_strategy=\"epoch\",\n",
    "    max_grad_norm=1.0,              \n",
    "    warmup_ratio=0.1,               \n",
    "    report_to=[\"tensorboard\"],      \n",
    "    save_total_limit=2,             \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in processed datasets:\n",
      "Training: [22542 20832 18318 12990]\n",
      "Validation: [266 277 285 172]\n",
      "\n",
      "Datasets validated successfully. Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14004' max='14004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14004/14004 1:25:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.649500</td>\n",
       "      <td>0.381315</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.870973</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.872000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.310500</td>\n",
       "      <td>0.167275</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.944956</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.184608</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.951991</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0961553662342c0b8c665bc32b84188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12eadcae0ff43e19008d72e82e6b010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label distribution in processed datasets:\n",
      "Training: [22542 20832 18318 12990]\n",
      "Validation: [266 277 285 172]\n",
      "\n",
      "Datasets validated successfully. Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='999' max='14004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  999/14004 05:55 < 1:17:13, 2.81 it/s, Epoch 0.21/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.197700</td>\n",
       "      <td>0.258356</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.946085</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 124\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_datasets():\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDatasets validated successfully. Starting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 124\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bhuva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bhuva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2536\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2537\u001b[0m ):\n\u001b[0;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2540\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, EarlyStoppingCallback\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    \n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    # Compute classification metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "    \n",
    "    # Convert numeric labels back to text for ROUGE and BLEU\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    pred_texts = [id2label[p] for p in predictions]\n",
    "    label_texts = [id2label[l] for l in labels]\n",
    "    \n",
    "    # Compute ROUGE and BLEU\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    \n",
    "    rouge = rouge_metric.compute(predictions=pred_texts, references=label_texts)\n",
    "    bleu = bleu_metric.compute(predictions=pred_texts, references=[[t] for t in label_texts])\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"rouge1\": rouge[\"rouge1\"],\n",
    "        \"rouge2\": rouge[\"rouge2\"],\n",
    "        \"rougeL\": rouge[\"rougeL\"],\n",
    "        \"bleu\": bleu[\"bleu\"]\n",
    "    }\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        inputs_without_labels = {k: v for k, v in inputs.items() if k != \"labels\"}\n",
    "        \n",
    "        outputs = model(**inputs_without_labels)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        if labels is not None:\n",
    "            # Create loss function with weights\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            loss = None\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        with self.compute_loss_context_manager():\n",
    "            loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1 and not self.deepspeed:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        return loss.detach()\n",
    "    \n",
    "def validate_datasets():\n",
    "    # Check if datasets are properly formatted\n",
    "    sample = train_dataset[0]\n",
    "    required_keys = ['input_ids', 'attention_mask', 'label']\n",
    "    \n",
    "    for key in required_keys:\n",
    "        if key not in sample:\n",
    "            raise ValueError(f\"Missing required key {key} in dataset\")\n",
    "            \n",
    "    # Verify label distribution\n",
    "    train_labels = [example['label'] for example in train_dataset]\n",
    "    valid_labels = [example['label'] for example in valid_dataset]\n",
    "    \n",
    "    print(\"\\nLabel distribution in processed datasets:\")\n",
    "    print(\"Training:\", np.bincount(train_labels))\n",
    "    print(\"Validation:\", np.bincount(valid_labels))\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    class_weights=class_weights\n",
    ")\n",
    "\n",
    "# Run validation and start training\n",
    "if validate_datasets():\n",
    "    print(\"\\nDatasets validated successfully. Starting training...\")\n",
    "    trainer.train()   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to C:\\Users\\bhuva\\Desktop\\projects_2025\\LLM\\best_model\n",
      "Evaluation Results:\n",
      "{'eval_loss': 0.2583555579185486, 'eval_accuracy': 0.946, 'eval_f1': 0.946084634519709, 'eval_rouge1': 0.946, 'eval_rouge2': 0.0, 'eval_rougeL': 0.946, 'eval_bleu': 0.0}\n",
      "Predictions on new tweets:\n",
      "Tweet: This game is absolutely amazing! --> Predicted Sentiment: Positive\n",
      "Tweet: I didn't like how the match turned out. --> Predicted Sentiment: Neutral\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = r\"C:\\Users\\bhuva\\Desktop\\projects_2025\\LLM\\best_model\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model and tokenizer saved to {model_path}\")\n",
    "\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\")\n",
    "print(eval_results)\n",
    "\n",
    "\n",
    "def predict(texts):\n",
    "    cleaned_texts = [preprocess_tweet(text) for text in texts]\n",
    "    inputs = tokenizer(cleaned_texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # move inputs to the device used by the model\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    predicted_labels = [id2label[int(pred)] for pred in predictions]\n",
    "    return predicted_labels\n",
    "\n",
    "# Example usage:\n",
    "new_tweets = [\n",
    "    \"This game is absolutely amazing!\",\n",
    "    \"I didn't like how the match turned out.\"\n",
    "]\n",
    "predictions = predict(new_tweets)\n",
    "print(\"Predictions on new tweets:\")\n",
    "for tweet, label in zip(new_tweets, predictions):\n",
    "    print(f\"Tweet: {tweet} --> Predicted Sentiment: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the model predictions.\n",
    "    This includes accuracy, F1, ROUGE, and BLEU scores.\n",
    "    \n",
    "    Parameters:\n",
    "        eval_pred: A tuple containing the model logits and the true labels.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary with keys: \"accuracy\", \"f1\", \"rouge1\", \"rouge2\", \"rougeL\", and \"bleu\".\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import evaluate\n",
    "\n",
    "    # Load metrics\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Compute classification metrics\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "\n",
    "    # Map numeric labels back to text since ROUGE and BLEU are text-oriented\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    pred_texts = [id2label[p] for p in predictions]\n",
    "    label_texts = [id2label[l] for l in labels]\n",
    "\n",
    "    # Compute ROUGE scores (this computes multiple ROUGE metrics)\n",
    "    rouge_results = rouge_metric.compute(predictions=pred_texts, references=label_texts)\n",
    "    \n",
    "    # Compute BLEU score\n",
    "    bleu_results = bleu_metric.compute(\n",
    "        predictions=pred_texts,\n",
    "        references=[[ref] for ref in label_texts]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"rouge1\": rouge_results[\"rouge1\"],\n",
    "        \"rouge2\": rouge_results[\"rouge2\"],\n",
    "        \"rougeL\": rouge_results[\"rougeL\"],\n",
    "        \"bleu\": bleu_results[\"bleu\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Evaluation Scores ==============================\n",
      "eval_loss      : 0.2584\n",
      "eval_accuracy  : 0.9460\n",
      "eval_f1        : 0.9461\n",
      "eval_rouge1    : 0.9460\n",
      "eval_rouge2    : 0.0000\n",
      "eval_rougeL    : 0.9460\n",
      "eval_bleu      : 0.0000\n",
      "\n",
      "============================== Classification Report ==============================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Irrelevant       0.96      0.91      0.93       172\n",
      "    Negative       0.98      0.94      0.96       266\n",
      "     Neutral       0.91      0.96      0.94       285\n",
      "    Positive       0.94      0.96      0.95       277\n",
      "\n",
      "    accuracy                           0.95      1000\n",
      "   macro avg       0.95      0.94      0.94      1000\n",
      "weighted avg       0.95      0.95      0.95      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\" * 30, \"Evaluation Scores\", \"=\" * 30)\n",
    "for metric, score in eval_results.items():\n",
    "    print(f\"{metric:15s}: {score:,.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "predictions_output = trainer.predict(valid_dataset)\n",
    "logits = predictions_output.predictions\n",
    "labels = predictions_output.label_ids\n",
    "\n",
    "preds = np.argmax(logits, axis=1)\n",
    "\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "true_labels_text = [id2label[label] for label in labels]\n",
    "pred_labels_text = [id2label[pred] for pred in preds]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 30 + \" Classification Report \" + \"=\" * 30)\n",
    "print(classification_report(true_labels_text, pred_labels_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
